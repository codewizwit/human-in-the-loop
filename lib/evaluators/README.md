# Evaluators

Quality assurance tools that validate AI-generated outputs against defined criteria.

## What's an Evaluator?

Evaluators check AI outputs for:

- Code quality and standards
- Security vulnerabilities
- Documentation completeness
- Performance impact
- Test coverage

## Usage

Evaluators run automatically in CI/CD pipelines or can be invoked manually via the CLI.

## Structure

```
evaluators/
├── code-quality/
├── security/
└── documentation/
```

## Contributing

See [CONTRIBUTING.md](../../CONTRIBUTING.md) for guidelines.

---

**Human-in-the-Loop by codewizwit**
